{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9b991d",
   "metadata": {},
   "source": [
    "# Project 2: User Assessments Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f769d76",
   "metadata": {},
   "source": [
    "## PART 1: Introduction\n",
    "### **Business Case**\n",
    "- A highly reputable tech firm offers a state of the training & assessment platform enabeling the new radical shift from a highly scaleable and highly modular-digital platform. Many publishers have taken notice and have requested to utilize the platform assuming we can demonstrate a secure, reliable, and robust data engineering pipeline that's easily quireyable for their data sceintists. \n",
    "\n",
    "### **Goal:** \n",
    "Develop and demonstrate a basic data pipeline to transmit and distribute event messages into kaufka and analyzed using Spark\n",
    "\n",
    "### Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e17cd",
   "metadata": {},
   "source": [
    "### PART II: The Process Steps\n",
    "In this section, I have documented all the major steps in setting up the data pipeline all from the command line terminal.   \n",
    "I have annotated each step with the following:    \n",
    "--1. A brief description of the step  \n",
    "--2. The code used to execute in the cli  \n",
    "--3 The output copied from the command line terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483f3d1",
   "metadata": {},
   "source": [
    "**Step 1: Initial Set up**  \n",
    "\n",
    "- Make directory\n",
    "- Create docker-compose file. We will be using the same file from the week 8 async. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5853d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make directory and \n",
    "mkdir w205/project-2-dtrinidad002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ece4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the yml file from course contents\n",
    "cp ~/w205/course-content//08-Querying-Data/docker-compose.yml .\n",
    "\n",
    "# read docker-compose.yml \n",
    "cat docker-compose.yml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82dfff63",
   "metadata": {},
   "source": [
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    #ports:\n",
    "    #- \"8888:8888\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b885c",
   "metadata": {},
   "source": [
    "**Step 2: Get Data File**  \n",
    "-code for getting data below, followed by the output from the console  \n",
    "-check the data file and verify it's not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b20fd063",
   "metadata": {},
   "source": [
    "> curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
    "100 9096k  100 9096k    0     0  27.1M      0 --:--:-- --:--:-- --:--:-- 27.1M\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
    "100 9096k  100 9096k    0     0  32.9M      0 --:--:-- --:--:-- --:--:-- 32.9M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40c0d8",
   "metadata": {},
   "source": [
    "**Step 3: Spin Up Docker Container Cluster**    \n",
    "We will be using Kafka, Spark, Zookeeper, Cloudera and Mids containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd142189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate cluster container\n",
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34aac711",
   "metadata": {},
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose up -d\n",
    "Creating network \"project-2-dtrinidad002_default\" with the default driver\n",
    "Creating project-2-dtrinidad002_zookeeper_1 ... done\n",
    "Creating project-2-dtrinidad002_mids_1      ... done\n",
    "Creating project-2-dtrinidad002_cloudera_1  ... done\n",
    "Creating project-2-dtrinidad002_kafka_1     ... done\n",
    "Creating project-2-dtrinidad002_spark_1     ... done\n",
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify containers are running \n",
    "docker-compose ps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "297a13b6",
   "metadata": {},
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose ps\n",
    "               Name                           Command            State                                         Ports                                       \n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "project-2-dtrinidad002_cloudera_1    cdh_startup_script.sh       Up      11000/tcp, 11443/tcp, 19888/tcp, 50070/tcp, 8020/tcp, 8088/tcp, 8888/tcp, 9090/tcp\n",
    "project-2-dtrinidad002_kafka_1       /etc/confluent/docker/run   Up      29092/tcp, 9092/tcp                                                               \n",
    "project-2-dtrinidad002_mids_1        /bin/bash                   Up      8888/tcp                                                                          \n",
    "project-2-dtrinidad002_spark_1       docker-entrypoint.sh bash   Up                                                                                        \n",
    "project-2-dtrinidad002_zookeeper_1   /etc/confluent/docker/run   Up      2181/tcp, 2888/tcp, 32181/tcp, 3888/tcp                                           \n",
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8380be8",
   "metadata": {},
   "source": [
    "**Step 4: Set up Kaufka logs**  \n",
    "-We will run this in a separate terminal window  \n",
    "-Snippet of the logs running on 2nd terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose logs -f kafka\n",
    "Attaching to project-2-dtrinidad002_kafka_1\n",
    "kafka_1      | ===> User\n",
    "kafka_1      | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\n",
    "kafka_1      | ===> Configuring ...\n",
    "kafka_1      | ===> Running preflight checks ... \n",
    "kafka_1      | ===> Check if /var/lib/kafka/data is writable ...\n",
    "kafka_1      | ===> Check if Zookeeper is healthy ...\n",
    "kafka_1      | SLF4J: Class path contains multiple SLF4J bindings.\n",
    "kafka_1      | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "kafka_1      | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "kafka_1      | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "kafka_1      | SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]\n",
    "kafka_1      | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT\n",
    "kafka_1      | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=d0106b002e81\n",
    "kafka_1      | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.12\n",
    "kafka_1      | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.\n",
    "kafka_1      | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540b793",
   "metadata": {},
   "source": [
    "**Step 5: Check Hadoop tmp directory to verify what we want to write is not already there**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the inital hadoop filings\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef814637",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "Found 2 items\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-02 04:06 /tmp/hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945c383",
   "metadata": {},
   "source": [
    "***NOTES: we see that upon executing the hadoop folder we see 2 items in there**\n",
    "-These files are standard and will always show up. \n",
    "-We have confirmed that there are no other files in the temp folder that is not supposed to be there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e550f0",
   "metadata": {},
   "source": [
    "**Step 6: Create Topic \"assessment-attepmts\"**\n",
    "\n",
    "***why did i choose to name the topic \"assessment-attempts\"?***\n",
    "-I called the topic assessment-attempts in order to avoid ambiguity. Since the raw JSON file contains exam/assessments and user data, I chose to name the topic as such. It's good practice to name apply descriptive names to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ced42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topic \"assessment-attempts\"\n",
    "docker-compose exec kafka kafka-topics --create --topic assessment_attempts --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec kafka kafka-topics --create --topic assessment_attempts --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
    "Created topic assessment_attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314c770",
   "metadata": {},
   "source": [
    "**Step 7: Verify newly created kaufka topic \"assessment-attempts\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0aa1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe topic assessment-attempts\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessment_attempts --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9790ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec kafka kafka-topics --describe --topic assessment_attempts --zookeeper zookeeper:32181\n",
    "Topic: assessment_attempts      TopicId: y2dLSD4CRG2RlIwuEuLsrQ PartitionCount: 1       ReplicationFactor: 1    Configs: \n",
    "        Topic: assessment_attempts      Partition: 0    Leader: 1       Replicas: 1     Isr: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29210f1",
   "metadata": {},
   "source": [
    "**Step 8: Check how many entries are there in the JSON file**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c12260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to count entries via mids container\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-dtrinidad002/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | wc -l\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec mids bash -c \"cat /w205/project-2-dtrinidad002/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | wc -l\"\n",
    "\n",
    "3280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821bec2",
   "metadata": {},
   "source": [
    "There are ***3280*** entries in the nested json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5223c0f",
   "metadata": {},
   "source": [
    "**Step 9: Publish event messages to Kafka topic**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc77402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kafkacat to produce test messages to the \"assessment-attempts\" topic\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-dtrinidad002/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessment_attempts && echo 'Produced 3280 messages to topic assessment_attempts.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec mids bash -c \"cat /w205/project-2-dtrinidad002/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessment_attempts && echo 'Produced 3280 messages to topic assessment_attempts.'\"\n",
    "\n",
    "Produced 3280 messages to topic assessment_attempts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa3e21",
   "metadata": {},
   "source": [
    "**Step 10: Check messages sent to kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to print one message\n",
    "docker-compose exec kafka kafka-console-consumer --bootstrap-server kafka:29092 --topic assessment_attempts --from-beginning --max-messages 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c5bb462",
   "metadata": {},
   "source": [
    "# jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec kafka kafka-console-consumer --bootstrap-server kafka:29092 --topic assessment_attempts --from-beginning --max-messages 1\n",
    "{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}\n",
    "Processed a total of 1 messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587d32b",
   "metadata": {},
   "source": [
    "**Step 11: Run Pyspark**  \n",
    "- Spark will be used to consume messages that are published to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Pyspark\n",
    "docker-compose exec spark pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6872612",
   "metadata": {},
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec spark pyspark\n",
    "Python 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58) \n",
    "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "21/11/02 04:57:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "21/11/02 04:57:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n",
    "21/11/02 04:57:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
    "21/11/02 04:57:55 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.1 (default, May 11 2017 13:09:58)\n",
    "SparkSession available as 'spark'.\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc083e0d",
   "metadata": {},
   "source": [
    "**Step 12:Consume messages from the \"assessment_attempt\" topic into Spak, then cache to suppress warning messages**\n",
    "\n",
    "-json entries will turn into a spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessment_attempts\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()\n",
    "\n",
    "#cache\n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83554f2",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessment_attempts\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()\n",
    "\n",
    ">>> raw_assessments.cache()DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d1a59",
   "metadata": {},
   "source": [
    "**Step 13: Check the schema of \"raw_assessments\"**\n",
    "\n",
    "-This raw data set is immutable, and can't be altered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Schema  \n",
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff86193",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73ce19ff",
   "metadata": {},
   "source": [
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70331a",
   "metadata": {},
   "source": [
    "**Step 14: Convert the key and values into strings**\n",
    "    \n",
    "-In order to make the key and value readable, we need to convert them to strings. We would have to create a new data frame \"assessments\"\n",
    "which will be a direct copy of the raw_assessments data frame, that we will need to mutate as we aim to make this dataset quireyable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e479eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to convert key and value as strings\n",
    "assessments = raw_assessments.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "#Display assessments data frame\n",
    "assessments.show()\n",
    "\n",
    "#Double check the total number of messages have not changed\n",
    "assessments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4168b",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> assessments = raw_assessments.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    ">>> assessments.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bab57a04",
   "metadata": {},
   "source": [
    "+----+--------------------+\n",
    "| key|               value|\n",
    "+----+--------------------+\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "|null|{\"keen_timestamp\"...|\n",
    "+----+--------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> assessments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "3280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46c282",
   "metadata": {},
   "source": [
    "**Step 15: Save the \"assessment\" df into hdfs as a parquet file**\n",
    "- Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types.  This approach is best especially for those queries that need to read certain columns from a large table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "assessments.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d96dabb",
   "metadata": {},
   "source": [
    ">>> assessments.write.parquet(\"/tmp/assessments\")\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc43d5b",
   "metadata": {},
   "source": [
    "***(Open another terminal and verify the file in hdfs)***  \n",
    "-You can see \"assessments\" was added to hadoop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bedbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08788945",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec cloudera hadoop fs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778956b6",
   "metadata": {},
   "source": [
    "-ls /tmp/\n",
    "Found 3 items\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-02 05:27 /tmp/assessments\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-02 04:06 /tmp/hive\n",
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0767c4e",
   "metadata": {},
   "source": [
    "**Step 16: Extract JSON fields**\n",
    "- below you can see the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create extracted_assessments df\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "extracted_assessments.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> extracted_assessments.show(3)\n",
    "\n",
    "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
    "|        base_exam_id|certification|           exam_name|  keen_created_at|             keen_id|   keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
    "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...|1516717442.735266|5a6745820eb8ab000...|1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...|1516717377.639827|5a674541ab6b0a000...|1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738973.653394|5a67999d3ed3e3000...|1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
    "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
    "only showing top 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba3aea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
    "|        base_exam_id|certification|           exam_name|  keen_created_at|             keen_id|   keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
    "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...|1516717442.735266|5a6745820eb8ab000...|1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...|1516717377.639827|5a674541ab6b0a000...|1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738973.653394|5a67999d3ed3e3000...|1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
    "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
    "only showing top 3 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614cc07",
   "metadata": {},
   "source": [
    "***(Check Schema of the \"extracted_assessments\")***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b303c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd7519ca",
   "metadata": {},
   "source": [
    "root\n",
    " |-- base_exam_id: string (nullable = true)\n",
    " |-- certification: string (nullable = true)\n",
    " |-- exam_name: string (nullable = true)\n",
    " |-- keen_created_at: string (nullable = true)\n",
    " |-- keen_id: string (nullable = true)\n",
    " |-- keen_timestamp: string (nullable = true)\n",
    " |-- max_attempts: string (nullable = true)\n",
    " |-- sequences: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: array (valueContainsNull = true)\n",
    " |    |    |-- element: map (containsNull = true)\n",
    " |    |    |    |-- key: string\n",
    " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
    " |-- started_at: string (nullable = true)\n",
    " |-- user_exam_id: string (nullable = true)\n",
    "\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3ba2f",
   "metadata": {},
   "source": [
    "***(Can we expand the nested sequences further?)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "sequences = spark.read.json(assessments.rdd.map(lambda x: x.value))\n",
    "sequences.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9188d8da",
   "metadata": {},
   "source": [
    ">>> sequences = spark.read.json(assessments.rdd.map(lambda x: x.value))\n",
    ">>> sequences.printSchema()\n",
    "root\n",
    " |-- base_exam_id: string (nullable = true)\n",
    " |-- certification: string (nullable = true)\n",
    " |-- exam_name: string (nullable = true)\n",
    " |-- keen_created_at: string (nullable = true)\n",
    " |-- keen_id: string (nullable = true)\n",
    " |-- keen_timestamp: string (nullable = true)\n",
    " |-- max_attempts: string (nullable = true)\n",
    " |-- sequences: struct (nullable = true)\n",
    " |    |-- attempt: long (nullable = true)\n",
    " |    |-- counts: struct (nullable = true)\n",
    " |    |    |-- all_correct: boolean (nullable = true)\n",
    " |    |    |-- correct: long (nullable = true)\n",
    " |    |    |-- incomplete: long (nullable = true)\n",
    " |    |    |-- incorrect: long (nullable = true)\n",
    " |    |    |-- submitted: long (nullable = true)\n",
    " |    |    |-- total: long (nullable = true)\n",
    " |    |    |-- unanswered: long (nullable = true)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- questions: array (nullable = true)\n",
    " |    |    |-- element: struct (containsNull = true)\n",
    " |    |    |    |-- id: string (nullable = true)\n",
    " |    |    |    |-- options: array (nullable = true)\n",
    " |    |    |    |    |-- element: struct (containsNull = true)\n",
    " |    |    |    |    |    |-- at: string (nullable = true)\n",
    " |    |    |    |    |    |-- checked: boolean (nullable = true)\n",
    " |    |    |    |    |    |-- correct: boolean (nullable = true)\n",
    " |    |    |    |    |    |-- id: string (nullable = true)\n",
    " |    |    |    |    |    |-- submitted: long (nullable = true)\n",
    " |    |    |    |-- user_correct: boolean (nullable = true)\n",
    " |    |    |    |-- user_incomplete: boolean (nullable = true)\n",
    " |    |    |    |-- user_result: string (nullable = true)\n",
    " |    |    |    |-- user_submitted: boolean (nullable = true)\n",
    " |-- started_at: string (nullable = true)\n",
    " |-- user_exam_id: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50336d",
   "metadata": {},
   "source": [
    "***NOTES:*** We can see that there is a lot of nested data in the sequences root key value, however, it's unlikely that it's useful for queriying. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193557d",
   "metadata": {},
   "source": [
    "**Step 17: Make temp table in spark called \"assessments\"**\n",
    "\n",
    "-We will use the extracted_assessments data frame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c52ee059",
   "metadata": {},
   "source": [
    ">>> extracted_assessments.registerTempTable('assessments')\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43259fed",
   "metadata": {},
   "source": [
    "**Step 18: Run Spark SQL quiries against the temp table \"assessments\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1001714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "spark.sql(\"select user_exam_id, base_exam_id, exam_name, assessments.sequences.questions, max_attempts from assessments\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16f1e0e6",
   "metadata": {},
   "source": [
    ">>> spark.sql(\"select user_exam_id, base_exam_id, exam_name, assessments.sequences.questions, max_attempts from assessments\").show()\n",
    "+--------------------+--------------------+--------------------+--------------------+------------+\n",
    "|        user_exam_id|        base_exam_id|           exam_name|           questions|max_attempts|\n",
    "+--------------------+--------------------+--------------------+--------------------+------------+\n",
    "|6d4089e4-bde5-4a2...|37f0a30a-7464-11e...|Normal Forms and ...|[Map(user_incompl...|         1.0|\n",
    "|2fec1534-b41f-441...|37f0a30a-7464-11e...|Normal Forms and ...|[Map(user_incompl...|         1.0|\n",
    "|8edbc8a8-4d26-429...|4beeac16-bb83-4d5...|The Principles of...|[Map(user_incompl...|         1.0|\n",
    "|c0ee680e-8892-4e6...|4beeac16-bb83-4d5...|The Principles of...|[Map(user_incompl...|         1.0|\n",
    "|e4525b79-7904-405...|6442707e-7488-11e...|Introduction to B...|[Map(user_incompl...|         1.0|\n",
    "|3186dafa-7acf-47e...|8b4488de-43a5-4ff...|        Learning Git|[Map(user_incompl...|         1.0|\n",
    "|48d88326-36a3-4cb...|e1f07fac-5566-4fd...|Git Fundamentals ...|[Map(user_incompl...|         1.0|\n",
    "|bb152d6b-cada-41e...|7e2e0b53-a7ba-458...|Introduction to P...|[Map(user_incompl...|         1.0|\n",
    "|70073d6f-ced5-4d0...|1a233da8-e6e5-48a...|Intermediate Pyth...|[Map(user_incompl...|         1.0|\n",
    "|9eb6d4d6-fd1f-4f3...|7e2e0b53-a7ba-458...|Introduction to P...|[Map(user_incompl...|         1.0|\n",
    "|093f1337-7090-457...|4cdf9b5f-fdb7-4a4...|A Practical Intro...|[Map(user_incompl...|         1.0|\n",
    "|0f576abb-958a-4c0...|e1f07fac-5566-4fd...|Git Fundamentals ...|[Map(user_incompl...|         1.0|\n",
    "|0c18f48c-0018-450...|87b4b3f9-3a86-435...|Introduction to M...|[Map(user_incompl...|         1.0|\n",
    "|b38ac9d8-eef9-495...|a7a65ec6-77dc-480...|   Python Epiphanies|[Map(user_incompl...|         1.0|\n",
    "|bbc9865f-88ef-42e...|7e2e0b53-a7ba-458...|Introduction to P...|[Map(user_incompl...|         1.0|\n",
    "|8a0266df-02d7-44e...|e5602ceb-6f0d-11e...|Python Data Struc...|[Map(user_incompl...|         1.0|\n",
    "|95d4edb1-533f-445...|e5602ceb-6f0d-11e...|Python Data Struc...|[Map(user_incompl...|         1.0|\n",
    "|f9bc1eff-7e54-42a...|f432e2e3-7e3a-4a7...|Working with Algo...|[Map(user_incompl...|         1.0|\n",
    "|dc4b35a7-399a-4bd...|76a682de-6f0c-11e...|Learning iPython ...|[Map(user_incompl...|         1.0|\n",
    "|d0f8249a-597e-4e1...|a7a65ec6-77dc-480...|   Python Epiphanies|[Map(user_incompl...|         1.0|\n",
    "+--------------------+--------------------+--------------------+--------------------+------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97778a3e",
   "metadata": {},
   "source": [
    "**Step 19: Save quirey to another partuet file in hdfs**  \n",
    "-This data frame is a very basic and includes just the root columns (except the keen tables)  \n",
    "-From Step 18, we can see that the questions sequence is still heavily nested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ed65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to created baseLevel_assessments data frame\n",
    "baseLevel_assessments_info = spark.sql(\"SELECT user_exam_id, base_exam_id, exam_name, assessments.sequences.questions, max_attempts from assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the quirey results to parquet file in hadoop\n",
    "baseLevel_assessments_info.write.parquet(\"/tmp/some_assessments_info\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3476169",
   "metadata": {},
   "source": [
    ">>> baseLevel_assessments_info = spark.sql(\"SELECT user_exam_id, base_exam_id, exam_name, assessments.sequences.questions, max_attempts from assessments\")\n",
    ">>> baseLevel_assessments_info.write.parquet(\"/tmp/some_assessments_info\")\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fcbbc",
   "metadata": {},
   "source": [
    "**Step 20 check parquet file in a different terminal window**  \n",
    "-You can see the \"baseLevel_assessments_info\" quirey was added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df798cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fcc71d7",
   "metadata": {},
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "Found 4 items\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-02 05:27 /tmp/assessments\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-02 06:25 /tmp/baseLevel_assessments_info\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-02 04:06 /tmp/hive\n",
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d48c0",
   "metadata": {},
   "source": [
    "**Step 21: Extract questions field**  \n",
    "\n",
    "-For this step we will use recursion \n",
    "-The Nested Key Values are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a8e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lambda_for_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    list = []\n",
    "    count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        count += 1\n",
    "        dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"count\" : count, \"id\" : l[\"id\"]}\n",
    "        list.append(dict)\n",
    "    return list\n",
    "\n",
    "questions = assessments.rdd.flatMap(lambda_for_questions).toDF()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83a64b3",
   "metadata": {},
   "source": [
    ">>> def lambda_for_questions(x):\n",
    "...     raw_dict = json.loads(x.value)\n",
    "...     list = []\n",
    "...     count = 0\n",
    "...     for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "...         count += 1\n",
    "...         dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"count\" : count, \"id\" : l[\"id\"]}\n",
    "...         list.append(dict)\n",
    "...     return list\n",
    "... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.registerTempTable('questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT id, count FROM questions\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f029740",
   "metadata": {},
   "source": [
    ">>> questions = assessments.rdd.flatMap(lambda_for_questions).toDF()\n",
    "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
    "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n",
    ">>> questions.registerTempTable('questions')\n",
    ">>> spark.sql(\"SELECT id, count FROM questions\").show()\n",
    "+--------------------+-----+\n",
    "|                  id|count|\n",
    "+--------------------+-----+\n",
    "|7a2ed6d3-f492-49b...|    1|\n",
    "|bbed4358-999d-446...|    2|\n",
    "|e6ad8644-96b1-461...|    3|\n",
    "|95194331-ac43-454...|    4|\n",
    "|95194331-ac43-454...|    1|\n",
    "|bbed4358-999d-446...|    2|\n",
    "|e6ad8644-96b1-461...|    3|\n",
    "|7a2ed6d3-f492-49b...|    4|\n",
    "|b9ff2e88-cf9d-4bd...|    1|\n",
    "|bec23e7b-4870-49f...|    2|\n",
    "|1ba75b31-64a4-4bd...|    3|\n",
    "|1f7c5def-904b-483...|    4|\n",
    "|1f7c5def-904b-483...|    1|\n",
    "|bec23e7b-4870-49f...|    2|\n",
    "|1ba75b31-64a4-4bd...|    3|\n",
    "|b9ff2e88-cf9d-4bd...|    4|\n",
    "|620c924f-6bd8-11e...|    1|\n",
    "|f4bc7618-76ac-11e...|    2|\n",
    "|42d6d026-6bd8-11e...|    3|\n",
    "|8372371c-6bd8-11e...|    4|\n",
    "+--------------------+-----+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the \"keen_id\" column from the assessments df with the questions df\n",
    "spark.sql(\"SELECT q.keen_id, a.keen_timestamp, q.id FROM assessments a JOIN questions q on a.keen_id = q.keen_id\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb325198",
   "metadata": {},
   "source": [
    ">>> spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from assessments a join questions q on a.keen_id = q.keen_id\").show()\n",
    "+--------------------+------------------+--------------------+\n",
    "|             keen_id|    keen_timestamp|                  id|\n",
    "+--------------------+------------------+--------------------+\n",
    "|5a17a67efa1257000...|1511499390.3836269|803fc93f-7eb2-412...|\n",
    "|5a17a67efa1257000...|1511499390.3836269|f3cb88cc-5b79-41b...|\n",
    "|5a17a67efa1257000...|1511499390.3836269|32fe7d8d-6d89-4db...|\n",
    "|5a17a67efa1257000...|1511499390.3836269|5c34cf19-8cfd-4f5...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|0603e6f4-c3f9-4c2...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|26a06b88-2758-45b...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|25b6effe-79b0-4c4...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|6de03a9b-2a78-46b...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|aaf39991-fa83-470...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|aab2e817-73dc-4ff...|\n",
    "|5a26ee9cbf5ce1000...|1512500892.4166169|58253a21-a603-4d6...|\n",
    "|5a29dcac74b662000...|1512692908.8423469|fb07b16e-84a2-465...|\n",
    "|5a29dcac74b662000...|1512692908.8423469|51b6a32e-80cb-4d5...|\n",
    "|5a29dcac74b662000...|1512692908.8423469|d10c5c1f-956a-429...|\n",
    "|5a29dcac74b662000...|1512692908.8423469|54fabf21-1486-41c...|\n",
    "|5a29dcac74b662000...|1512692908.8423469|5cc84ec3-713c-45f...|\n",
    "|5a2fdab0eabeda000...|1513085616.2275269|b0666bc4-2aa8-4cf...|\n",
    "|5a2fdab0eabeda000...|1513085616.2275269|164c41b0-6bd8-11e...|\n",
    "|5a2fdab0eabeda000...|1513085616.2275269|42d6d026-6bd8-11e...|\n",
    "|5a2fdab0eabeda000...|1513085616.2275269|8372371c-6bd8-11e...|\n",
    "+--------------------+------------------+--------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6206fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display questions df with the added keen_id from the assessments df\n",
    "questions.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8979b047",
   "metadata": {},
   "source": [
    ">>> questions.show()\n",
    "+-----+--------------------+--------------------+\n",
    "|count|                  id|             keen_id|\n",
    "+-----+--------------------+--------------------+\n",
    "|    1|7a2ed6d3-f492-49b...|5a6745820eb8ab000...|\n",
    "|    2|bbed4358-999d-446...|5a6745820eb8ab000...|\n",
    "|    3|e6ad8644-96b1-461...|5a6745820eb8ab000...|\n",
    "|    4|95194331-ac43-454...|5a6745820eb8ab000...|\n",
    "|    1|95194331-ac43-454...|5a674541ab6b0a000...|\n",
    "|    2|bbed4358-999d-446...|5a674541ab6b0a000...|\n",
    "|    3|e6ad8644-96b1-461...|5a674541ab6b0a000...|\n",
    "|    4|7a2ed6d3-f492-49b...|5a674541ab6b0a000...|\n",
    "|    1|b9ff2e88-cf9d-4bd...|5a67999d3ed3e3000...|\n",
    "|    2|bec23e7b-4870-49f...|5a67999d3ed3e3000...|\n",
    "|    3|1ba75b31-64a4-4bd...|5a67999d3ed3e3000...|\n",
    "|    4|1f7c5def-904b-483...|5a67999d3ed3e3000...|\n",
    "|    1|1f7c5def-904b-483...|5a6799694fc7c7000...|\n",
    "|    2|bec23e7b-4870-49f...|5a6799694fc7c7000...|\n",
    "|    3|1ba75b31-64a4-4bd...|5a6799694fc7c7000...|\n",
    "|    4|b9ff2e88-cf9d-4bd...|5a6799694fc7c7000...|\n",
    "|    1|620c924f-6bd8-11e...|5a6791e824fccd000...|\n",
    "|    2|f4bc7618-76ac-11e...|5a6791e824fccd000...|\n",
    "|    3|42d6d026-6bd8-11e...|5a6791e824fccd000...|\n",
    "|    4|8372371c-6bd8-11e...|5a6791e824fccd000...|\n",
    "+-----+--------------------+--------------------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa67a1",
   "metadata": {},
   "source": [
    "**Step 23: Write quirey to parquet file** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.write.parquet(\"/tmp/questions\")\n",
    "\n",
    "#check temp files in hadoop on another terminal\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ac943a5",
   "metadata": {},
   "source": [
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ docker-compose exec cloudera hadoop fs -ls /tmp\n",
    "Found 5 items\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-02 05:27 /tmp/assessments\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-02 06:25 /tmp/baseLevel_assessments_info\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-02 04:06 /tmp/hive\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-02 08:06 /tmp/questions\n",
    "jupyter@python-20210907-215615:~/w205/project-2-dtrinidad002$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65530e0",
   "metadata": {},
   "source": [
    "***Results***  \n",
    "We have successfully saved 3 new parquet files in hadoop  \n",
    "1.Assessments   \n",
    "2.BaseLevel_assessments_info  \n",
    "3.Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74068557",
   "metadata": {},
   "source": [
    "### Summary: \n",
    "\n",
    "We have successfully stepped through the process of demonstrating how to utilize docker clusters to consume and publish data streaming messages in Kafka. The JSON file contained user-assessment data which contained 10 root key variables and many nested key variables under the questions column. We also demonstrated the use of Spark to consume messages and transform them as needed. We then stored these messgages into Hadoop (HDFS). Though we would love to work with flat data tables, examples like this are likely what we will come across in the real world. Docker clusters like what's demonstrated here are what make the scale up possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d84ab1",
   "metadata": {},
   "source": [
    "**Reccomendations**  \n",
    "Without any clear documentation to describe the data attributes, I can only make the following assumptions of the data schema: \n",
    "\n",
    "1. The base_exam_id is linked to a specific exam_name \n",
    "2. User_exam_id is unique and does not repeat within the data set\n",
    "3. Started_at column is formatted as (year-month-day-Time)\n",
    "4. Certification \"False\" means the individual did not pass\n",
    "\n",
    "Reccomendation 1: \n",
    "-Read deeper into the data attributes to get further context to understand the data. It's likely that some of my assumptions could be incorrect, or there may be data attributes i may have overlooked. \n",
    "\n",
    "Reccomendation 2:   \n",
    "-Document a standard best practice for our data pipeline. This includes naming conventions for topics and data frames. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ea0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
